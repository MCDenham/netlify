---
title: Understanding Confounding and Aliasing
author: Mike Denham
date: '2018-11-09'
slug: understanding-confounding-and-aliasing
categories: []
tags: []
header:
  caption: ''
  image: ''
---



<p>Statistical terminology can be confusing. The terms confounding and aliasing were introduced by different researchers at different times during the development of experimental design theory. The two terms were introduced in subtly different contexts. The term confounding was introduced in the late 1920s by Ronald Fisher in the context of blocking experiments. It was used to describe the situation of deliberately “confounding” effects due to differences in blocks with typically higher order interactions between experimental factors to keep the size of a block to a manageable number. Later on, in the 1940s when fractional factorial designs were introduced by Finney, the term “aliasing” was used to describe the situation when two different factorial effects (such as two separate two-way interactions) could not be estimated separately from one another using the observed results from the experiment. There is little practical difference between these two situations and over time it has become widespread practice for the two terms to be used interchangeably. We will use the term “aliasing” throughout this discussion.</p>
<p>Two effects are aliased if their estimates are correlated with each other^<sup>1</sup>^. In the extreme case of perfect correlation (±1) it becomes mathematically impossible to discern the impact of one effect from the other using the data from the experiment. Fractional factorial designs do this on purpose. They lead to designs in which pairs of effects are either completely aliased (i.e. the estimates are perfectly correlated) or they are not aliased at all (i.e. the estimates have a correlation of zero). For instance, resolution IV fractional factorials have the property that estimates of main effects are uncorrelated with those of any other main effects or two-factor interactions. At the same time estimates of two-factor interactions will be perfectly correlated with estimates for some other two-factor interactions and uncorrelated with others. Prior information about what are likely to be the important effects can sometimes be used to choose designs where potentially important effects are only aliased with effects which are scientifically unlikely to exist. This reliance on both the observed data and our prior knowledge allows us to obtain the required information using fewer experimental resources. Fractional factorial designs have the desirable feature that they lead to a clear distinction of when estimated effects are unaffected by the size of other effects and when they are not.</p>
<p>It is also possible to create designs in which estimates of effects are correlated but not perfectly so. These designs allow us to mathematically tease out separate effects because their estimates are not perfectly correlated. Such effects are sometimes referred to as partially aliased. This can lead to misunderstandings when reading the literature; some authors use aliasing or confounding when they really mean complete aliasing. In our view, aliasing can refer to either complete or partial aliasing. The phrase “not completely aliased” (or equivalently not completely confounded) does not mean “not aliased” (or not confounded). Definitive Screening Designs are an example of a class of experimental designs which have partial aliasing. In these designs, main effects estimates are not aliased with each other or with any two-factors interactions or quadratic effects. At the same time, two-factor interactions and quadratic effect estimates are partially aliased with each other. This partial aliasing makes interpretation of the uncertainty associated with effect estimates much more difficult. It also makes choosing an appropriate model more difficult. Even though pairs of effects may not be perfectly correlated with each other, it can still be mathematically impossible to distinguish between the overall impact of two different sets of effects.</p>
<hr>
<p>^<sup>1</sup>^This isn’t entirely true. I guess strictly speaking the two effects are aliased if the associated columns of the design matrix are not orthogonal. When we have two columns that are linearly dependent, we generally include one effect and not the other. The estimated effect is then biased. (We do not have an estimator for the other term, so we cannot talk about the correlation between them.) When we have partial aliasing and we fit both terms we have correlation but no bias due to the partially aliased effect.</p>
<p>One could think of this as dividing the design matrix into separate sub-matrices where <span class="math display">\[X_1\]</span> represents effects included in the model and <span class="math display">\[X_2\]</span> represents terms effects not included in the model. Note <span class="math display">\[X_2\]</span> can include any effect included in the model. These can be explicitly omitted effects as higher order terms not explicitly considered or even effects associated with unknown process factors.</p>
<p>@<span class="citation">@display:block</span>;text-align:center;<span class="math display">\[y = X_1{\vec\beta_1} + X_2\vec\beta_2 + \epsilon\]</span>@@</p>
<p>We estimate <span class="math display">\[\vec\beta_1\]</span> as <span class="math display">\[\hat \beta_1 = \left(X_1^TX_1\right)^{-1}X_1^Ty\]</span>, so that</p>
<p>@<span class="citation">@display:block</span>;text-align:center; <span class="math display">\[E\left[\hat \beta_1\right] = \vec\beta_1 + \left(X_1^TX_1\right)^{-1}X_1^TX_2\vec\beta_2\]</span>@@
and
@<span class="citation">@display:block</span>;text-align:center; <span class="math display">\[V\left[\hat \beta_1\right] = \left(X_1^TX_1\right)^{-1}\sigma^2.\]</span>@@</p>
<p>If <span class="math display">\[X_1^TX_2 = 0\]</span> then <span class="math display">\[\hat\beta_1\]</span> is an unbiased estimate regardless of the values of <span class="math display">\[\vec\beta_2\]</span>.</p>
<p>If <span class="math display">\[X_1^TX_2 \ne 0\]</span> then <span class="math display">\[\hat \beta_1\]</span> is a biased estimate unless <span class="math display">\[\vec\beta_2 = \vec0.\]</span></p>
<p>For the <span class="math display">\[j\]</span>th coefficient in <span class="math display">\[\hat \beta_1\]</span> (i.e. <span class="math display">\[\beta_{1j}\]</span>) to be unbiased for the corresponding coefficient in <span class="math display">\[\vec\beta_1\]</span>, for each value of <span class="math display">\[k\]</span>, one or other of <span class="math display">\[\left[\left(X_1^TX_1\right)^{-1}X_1^TX_2\right]_{jk} \]</span> and <span class="math display">\[\beta_{2k}\]</span> must be equal to zero.</p>
