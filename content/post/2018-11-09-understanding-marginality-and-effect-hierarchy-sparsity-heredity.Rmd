---
title: Understanding marginality and effect hierarchy, sparsity & heredity
author: Mike Denham
date: '2018-11-09'
slug: understanding-marginality-and-effect-hierarchy-sparsity-heredity
categories: []
tags: []
header:
  caption: ''
  image: ''
---

Did I mention that statistical terminology can be confusing?  This is another set of terms that seem to get used somewhat interchangeably.  Again I believe there are some subtle (and this time important) differences in their meaning and implications.
~McCullagh and Nelder used the term “marginality” in their seminal work on generalised linear models.  Their idea has its roots in Taylor series expansions.  
 
To illustrate let’s temporarily imagine a world with no statistical error.  We have a response variable $$y$$ whose value can be determined from the value of a single explanatory variable $$x$$ through a functional relationship $$y=f(x)$$.  Let us suppose that this function $$f(x)$$ is continuous and differentiable but not necessarily polynomial.  We can express $$f(x)$$ using an infinite Taylor series expansion about an arbitrary point $$x_0$$.
 
@@display:block;text-align:center;$$y = f(x_0) +  \left.{df(x)\over dx}\right|_{x_0} \times (x-x_0) +  {1\over 2!} \left.d^2 f(x)\over dx^2\right|_{x_0}\times(x-x_0)^2 + \ldots $$@@
 
 
We can create approximations of $$f(x)$$ using finite Taylor series expansions.  For example, a quadratic polynomial approximation would use
 
@@display:block;text-align:center;$$y= a_0  + b_0 \times (x-x_0) + c_0 \times (x-x_0)^2$$@@
 
with $$a_0=f(x_0 )$$; $$b_0= {df(x)\over dx}|_{x_0}$$; $$c_0={1\over 2!} {d^2 f(x)\over dx^2}|_{x_0}$$.
 
Now let’s suppose we have a pure quadratic relationship, $$y =\alpha +\gamma x^2$$.  In our Taylor series expansion, we have $$a_0  =\alpha +\gamma x_0^2$$,  $$b_0  =\gamma x_0$$ and $$c_0  =\gamma$$.  Assuming $$\gamma \ne 0$$, then the linear term is only zero when we choose $$x_0$$ to be zero.  In other words if we shift the “origin” of the $$x$$ variable the relationship ceases to be a purely quadratic one.  Since for most covariates the definition of what constitutes "zero" is arbitrary, it does not make any sense to omit the linear term when fitting the quadratic.  I believe, this is what ~McCullagh and Nelder mean when they refer to marginality.  Note that this interpretation has absolutely nothing to do with the statistical significance of the terms or their magnitude, it is about preserving the behaviour of the implied model under changes of location and scale in the covariate.
 
A similar argument can be applied in the context of factorial models and effects.  The principle of marginality drives a requirement to include relevant lower order terms when fitting higher order terms.  Except in rare circumstances where the "zero" setting of a factor is not arbitrary, fitting a two-factor interaction requires the inclusion of the corresponding main effects to preserve the behaviour of the model in changes of location.  Similarly, modelling three-factor interactions requires the inclusion of the relevant set of two-factor interactions and main effects. Again, this principle has nothing to do with believing the significance or magnitude of effects decreases with increasing order. [I've omitted the mathematical expressions for now, but who knows, I might put them in at some point - MCD]
 
A hierarchical model is one which obeys the principle of marginality.  When you only select BF and F terms in a half normal plot using DX10, DX10 issues a warning to say the model you have selected is not hierarchical.  It does not provide a warning if you select, F and BF terms if the magnitude of the estimated BF effect is bigger than the estimated main effects for B and F. 
 
On the other hand, the terms effect hierarchy, effect sparsity and effect heredity popularised by Hamada and Wu relate to the relative importance of effects and the abundance of relatively important effects.  The idea of effect hierarchy is that lower order effects are more likely to be important than higher order effects and that effects of the same order are equally likely to be important.  The idea of effect sparsity is essentially a restatement of the Pareto Principle in the context of experimental design. It posits that the number of relatively important effects in an experiment is small.  Finally effect heredity is more of a model fitting principle where a higher order effect can only be deemed significant if the relevant lower order effects are also significant.  Particularly in the context of two-factor interactions, this is sometimes referred to as the strong heredity principle to distinguish it from a relaxed version known as weak heredity where a two-factor interaction can be deemed significant if only one of the relevant main effects is significant.  
 
I am not clear whether significant is being used to mean practically important or whether it is being used to mean statistically significant. It seems to me that Hamada and Wu interpret it both ways as it suits them.  Hamada and Wu use the principles to motivate a proposed model selection strategy based on forward selection.  Forward selection has some well-known difficulties associated with adding terms one at a time and only if they are statistically significant at a threshold level.  These difficulties are perhaps the motivation for the concept of effect heredity based on statistical significance as it can then be used as a justification for the forward selection strategy.
 
I would argue that except in rare circumstances chosen models should obey M&Ns marginality principle.  Best possible subsets approaches using criteria such as AICc where the subset models considered obey the marginality principle combined with subject matter knowledge would seem to me to be far better approaches than relying on H&W’s effect heredity and forward selection.